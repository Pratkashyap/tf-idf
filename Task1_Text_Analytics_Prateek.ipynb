{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "import re\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to clean the data\n",
    "\n",
    "def clean_text(x):\n",
    "    y = x.lower()\n",
    "    y = re.sub(r'[^0-9a-z ]', '', y)\n",
    "    y = re.sub('\\s+', ' ', y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define UDF's used in the process\n",
    "\n",
    "clean_text_udf = F.udf(clean_text, StringType())\n",
    "dot_udf = F.udf(lambda x,y: float(x.dot(y)), DoubleType()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read and process the file\n",
    "\n",
    "def get_docs(path):\n",
    "    textFiles = sc.wholeTextFiles(path)\n",
    "    tmpFilesRDD = textFiles.map(lambda docs:(docs[0][docs[0].rfind('/')+1:], docs[1]))\n",
    "    \n",
    "    # Read all the docs in a dataframe. Clean the text of the docs\n",
    "    textFilesDF = tmpFilesRDD.toDF([\"doc_name\", \"text\"])\n",
    "    textFilesDF = textFilesDF.withColumn('text',clean_text_udf(F.col('text')))\n",
    "    return textFilesDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_docs('datafiles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|doc_name|                text|\n",
      "+--------+--------------------+\n",
      "|  f6.txt|project gutenberg...|\n",
      "|  f7.txt|there is one that...|\n",
      "|  f5.txt|gwendolen still s...|\n",
      "|  f4.txt|jack a severe chi...|\n",
      "|  f1.txt|the project guten...|\n",
      "|  f3.txt|algernon have you...|\n",
      "|  f2.txt|lady bracknell th...|\n",
      "|  f9.txt|my darling said h...|\n",
      "|  f8.txt|john says i musnt...|\n",
      "| f10.txt|i often wonder if...|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all the pipeline elements\n",
    "# Tokenize, remove stopwords\n",
    "\n",
    "tokenizer = Tokenizer().setInputCol(\"text\").setOutputCol(\"tokenized\")\n",
    "stopwords = [clean_text(x.strip()) for x in open('stopwords.txt','r')]\n",
    "stop_words_remover = StopWordsRemover(inputCol='tokenized',outputCol='tokenized_clean',stopWords=stopwords)\n",
    "\n",
    "train = tokenizer.transform(train)\n",
    "train = stop_words_remover.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+\n",
      "|doc_name|                text|           tokenized|     tokenized_clean|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "|  f6.txt|project gutenberg...|[project, gutenbe...|[project, gutenbe...|\n",
      "|  f7.txt|there is one that...|[there, is, one, ...|[commands, road, ...|\n",
      "|  f5.txt|gwendolen still s...|[gwendolen, still...|[gwendolen, stand...|\n",
      "|  f4.txt|jack a severe chi...|[jack, a, severe,...|[jack, severe, ch...|\n",
      "|  f1.txt|the project guten...|[the, project, gu...|[project, gutenbe...|\n",
      "|  f3.txt|algernon have you...|[algernon, have, ...|[algernon, told, ...|\n",
      "|  f2.txt|lady bracknell th...|[lady, bracknell,...|[lady, bracknell,...|\n",
      "|  f9.txt|my darling said h...|[my, darling, sai...|[darling, beg, sa...|\n",
      "|  f8.txt|john says i musnt...|[john, says, i, m...|[john, musnt, los...|\n",
      "| f10.txt|i often wonder if...|[i, often, wonder...|[wonder, windows,...|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make TF-IDF\n",
    "vectorizer = CountVectorizer(inputCol='tokenized_clean', outputCol='tf').fit(train)\n",
    "train = vectorizer.transform(train)\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\").fit(train)\n",
    "train = idf.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|doc_name|                text|           tokenized|     tokenized_clean|                  tf|               tfidf|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  f6.txt|project gutenberg...|[project, gutenbe...|[project, gutenbe...|(5375,[0,2,4,6,11...|(5375,[0,2,4,6,11...|\n",
      "|  f7.txt|there is one that...|[there, is, one, ...|[commands, road, ...|(5375,[2,11,14,17...|(5375,[2,11,14,17...|\n",
      "|  f5.txt|gwendolen still s...|[gwendolen, still...|[gwendolen, stand...|(5375,[0,1,2,3,4,...|(5375,[0,1,2,3,4,...|\n",
      "|  f4.txt|jack a severe chi...|[jack, a, severe,...|[jack, severe, ch...|(5375,[1,2,3,4,5,...|(5375,[1,2,3,4,5,...|\n",
      "|  f1.txt|the project guten...|[the, project, gu...|[project, gutenbe...|(5375,[0,1,2,3,5,...|(5375,[0,1,2,3,5,...|\n",
      "|  f3.txt|algernon have you...|[algernon, have, ...|[algernon, told, ...|(5375,[2,3,4,5,6,...|(5375,[2,3,4,5,6,...|\n",
      "|  f2.txt|lady bracknell th...|[lady, bracknell,...|[lady, bracknell,...|(5375,[1,2,3,4,6,...|(5375,[1,2,3,4,6,...|\n",
      "|  f9.txt|my darling said h...|[my, darling, sai...|[darling, beg, sa...|(5375,[2,11,17,20...|(5375,[2,11,17,20...|\n",
      "|  f8.txt|john says i musnt...|[john, says, i, m...|[john, musnt, los...|(5375,[2,6,11,17,...|(5375,[2,6,11,17,...|\n",
      "| f10.txt|i often wonder if...|[i, often, wonder...|[wonder, windows,...|(5375,[0,2,4,6,8,...|(5375,[0,2,4,6,8,...|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute normalized TF-IDF\n",
    "normalizer = Normalizer(inputCol=\"tfidf\", outputCol=\"tfidf_norm\")\n",
    "train = normalizer.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|doc_name|                text|           tokenized|     tokenized_clean|                  tf|               tfidf|          tfidf_norm|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  f6.txt|project gutenberg...|[project, gutenbe...|[project, gutenbe...|(5375,[0,2,4,6,11...|(5375,[0,2,4,6,11...|(5375,[0,2,4,6,11...|\n",
      "|  f7.txt|there is one that...|[there, is, one, ...|[commands, road, ...|(5375,[2,11,14,17...|(5375,[2,11,14,17...|(5375,[2,11,14,17...|\n",
      "|  f5.txt|gwendolen still s...|[gwendolen, still...|[gwendolen, stand...|(5375,[0,1,2,3,4,...|(5375,[0,1,2,3,4,...|(5375,[0,1,2,3,4,...|\n",
      "|  f4.txt|jack a severe chi...|[jack, a, severe,...|[jack, severe, ch...|(5375,[1,2,3,4,5,...|(5375,[1,2,3,4,5,...|(5375,[1,2,3,4,5,...|\n",
      "|  f1.txt|the project guten...|[the, project, gu...|[project, gutenbe...|(5375,[0,1,2,3,5,...|(5375,[0,1,2,3,5,...|(5375,[0,1,2,3,5,...|\n",
      "|  f3.txt|algernon have you...|[algernon, have, ...|[algernon, told, ...|(5375,[2,3,4,5,6,...|(5375,[2,3,4,5,6,...|(5375,[2,3,4,5,6,...|\n",
      "|  f2.txt|lady bracknell th...|[lady, bracknell,...|[lady, bracknell,...|(5375,[1,2,3,4,6,...|(5375,[1,2,3,4,6,...|(5375,[1,2,3,4,6,...|\n",
      "|  f9.txt|my darling said h...|[my, darling, sai...|[darling, beg, sa...|(5375,[2,11,17,20...|(5375,[2,11,17,20...|(5375,[2,11,17,20...|\n",
      "|  f8.txt|john says i musnt...|[john, says, i, m...|[john, musnt, los...|(5375,[2,6,11,17,...|(5375,[2,6,11,17,...|(5375,[2,6,11,17,...|\n",
      "| f10.txt|i often wonder if...|[i, often, wonder...|[wonder, windows,...|(5375,[0,2,4,6,8,...|(5375,[0,2,4,6,8,...|(5375,[0,2,4,6,8,...|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the query \n",
    "query = get_docs('query.txt')\n",
    "pipleine = Pipeline(stages=[tokenizer,stop_words_remover,vectorizer,idf,normalizer])\n",
    "query = pipleine.fit(query).transform(query)\n",
    "query = query.selectExpr(\"tfidf_norm as tfidf_query\")\n",
    "query = train.crossJoin(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|doc_name|                text|           tokenized|     tokenized_clean|                  tf|               tfidf|          tfidf_norm|         tfidf_query|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  f6.txt|project gutenberg...|[project, gutenbe...|[project, gutenbe...|(5375,[0,2,4,6,11...|(5375,[0,2,4,6,11...|(5375,[0,2,4,6,11...|(5375,[325,560],[...|\n",
      "|  f7.txt|there is one that...|[there, is, one, ...|[commands, road, ...|(5375,[2,11,14,17...|(5375,[2,11,14,17...|(5375,[2,11,14,17...|(5375,[325,560],[...|\n",
      "|  f5.txt|gwendolen still s...|[gwendolen, still...|[gwendolen, stand...|(5375,[0,1,2,3,4,...|(5375,[0,1,2,3,4,...|(5375,[0,1,2,3,4,...|(5375,[325,560],[...|\n",
      "|  f4.txt|jack a severe chi...|[jack, a, severe,...|[jack, severe, ch...|(5375,[1,2,3,4,5,...|(5375,[1,2,3,4,5,...|(5375,[1,2,3,4,5,...|(5375,[325,560],[...|\n",
      "|  f1.txt|the project guten...|[the, project, gu...|[project, gutenbe...|(5375,[0,1,2,3,5,...|(5375,[0,1,2,3,5,...|(5375,[0,1,2,3,5,...|(5375,[325,560],[...|\n",
      "|  f3.txt|algernon have you...|[algernon, have, ...|[algernon, told, ...|(5375,[2,3,4,5,6,...|(5375,[2,3,4,5,6,...|(5375,[2,3,4,5,6,...|(5375,[325,560],[...|\n",
      "|  f2.txt|lady bracknell th...|[lady, bracknell,...|[lady, bracknell,...|(5375,[1,2,3,4,6,...|(5375,[1,2,3,4,6,...|(5375,[1,2,3,4,6,...|(5375,[325,560],[...|\n",
      "|  f9.txt|my darling said h...|[my, darling, sai...|[darling, beg, sa...|(5375,[2,11,17,20...|(5375,[2,11,17,20...|(5375,[2,11,17,20...|(5375,[325,560],[...|\n",
      "|  f8.txt|john says i musnt...|[john, says, i, m...|[john, musnt, los...|(5375,[2,6,11,17,...|(5375,[2,6,11,17,...|(5375,[2,6,11,17,...|(5375,[325,560],[...|\n",
      "| f10.txt|i often wonder if...|[i, often, wonder...|[wonder, windows,...|(5375,[0,2,4,6,8,...|(5375,[0,2,4,6,8,...|(5375,[0,2,4,6,8,...|(5375,[325,560],[...|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|doc_name|\n",
      "+--------+\n",
      "|  f4.txt|\n",
      "|  f2.txt|\n",
      "|  f9.txt|\n",
      "|  f5.txt|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarity\n",
    "query = query.withColumn('similariy', dot_udf(\"tfidf_query\", \"tfidf_norm\"))\n",
    "query= query.filter(\"similariy > 0\").orderBy('similariy', ascending=False).select(\"doc_name\")\n",
    "query.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
